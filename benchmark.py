#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
pyutils æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿™ä¸ªè„šæœ¬æµ‹è¯• pyutils åº“ä¸­å…³é”®å‡½æ•°çš„æ€§èƒ½è¡¨ç°ã€‚
"""

import time
import sys
import os
import asyncio
from typing import Callable, Any

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from pyutils import array, string, math, object, function, async_utils


def benchmark(func: Callable, *args, iterations: int = 1000, **kwargs) -> dict:
    """åŸºå‡†æµ‹è¯•å‡½æ•°
    
    Args:
        func: è¦æµ‹è¯•çš„å‡½æ•°
        *args: å‡½æ•°å‚æ•°
        iterations: è¿­ä»£æ¬¡æ•°
        **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°
        
    Returns:
        åŒ…å«æ€§èƒ½ç»Ÿè®¡çš„å­—å…¸
    """
    times = []
    
    # é¢„çƒ­
    for _ in range(min(10, iterations // 10)):
        func(*args, **kwargs)
    
    # æ­£å¼æµ‹è¯•
    for _ in range(iterations):
        start_time = time.perf_counter()
        func(*args, **kwargs)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    total_time = sum(times)
    avg_time = total_time / len(times)
    min_time = min(times)
    max_time = max(times)
    
    return {
        'total_time': total_time,
        'avg_time': avg_time,
        'min_time': min_time,
        'max_time': max_time,
        'iterations': iterations,
        'ops_per_second': iterations / total_time if total_time > 0 else float('inf')
    }


async def async_benchmark(func: Callable, *args, iterations: int = 100, **kwargs) -> dict:
    """å¼‚æ­¥å‡½æ•°åŸºå‡†æµ‹è¯•
    
    Args:
        func: è¦æµ‹è¯•çš„å¼‚æ­¥å‡½æ•°
        *args: å‡½æ•°å‚æ•°
        iterations: è¿­ä»£æ¬¡æ•°
        **kwargs: å‡½æ•°å…³é”®å­—å‚æ•°
        
    Returns:
        åŒ…å«æ€§èƒ½ç»Ÿè®¡çš„å­—å…¸
    """
    times = []
    
    # é¢„çƒ­
    for _ in range(min(5, iterations // 10)):
        await func(*args, **kwargs)
    
    # æ­£å¼æµ‹è¯•
    for _ in range(iterations):
        start_time = time.perf_counter()
        await func(*args, **kwargs)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    total_time = sum(times)
    avg_time = total_time / len(times)
    min_time = min(times)
    max_time = max(times)
    
    return {
        'total_time': total_time,
        'avg_time': avg_time,
        'min_time': min_time,
        'max_time': max_time,
        'iterations': iterations,
        'ops_per_second': iterations / total_time if total_time > 0 else float('inf')
    }


def format_time(seconds: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 1e-6:
        return f"{seconds * 1e9:.2f} ns"
    elif seconds < 1e-3:
        return f"{seconds * 1e6:.2f} Î¼s"
    elif seconds < 1:
        return f"{seconds * 1e3:.2f} ms"
    else:
        return f"{seconds:.2f} s"


def print_benchmark_result(name: str, result: dict):
    """æ‰“å°åŸºå‡†æµ‹è¯•ç»“æœ"""
    print(f"  {name}:")
    print(f"    å¹³å‡æ—¶é—´: {format_time(result['avg_time'])}")
    print(f"    æœ€å°æ—¶é—´: {format_time(result['min_time'])}")
    print(f"    æœ€å¤§æ—¶é—´: {format_time(result['max_time'])}")
    print(f"    æ€»æ—¶é—´: {format_time(result['total_time'])}")
    print(f"    æ“ä½œ/ç§’: {result['ops_per_second']:.0f}")
    print(f"    è¿­ä»£æ¬¡æ•°: {result['iterations']}")
    print()


def benchmark_array_functions():
    """æµ‹è¯•æ•°ç»„å‡½æ•°æ€§èƒ½"""
    print("ğŸ”¢ æ•°ç»„å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®
    small_array = list(range(100))
    medium_array = list(range(1000))
    large_array = list(range(10000))
    
    # chunk å‡½æ•°æµ‹è¯•
    result = benchmark(array.chunk, medium_array, 10, iterations=1000)
    print_benchmark_result("chunk (1000 items, size=10)", result)
    
    # unique å‡½æ•°æµ‹è¯•
    duplicated_array = small_array * 3  # åˆ›å»ºé‡å¤æ•°ç»„
    result = benchmark(array.unique, duplicated_array, iterations=1000)
    print_benchmark_result("unique (300 items with duplicates)", result)
    
    # shuffle å‡½æ•°æµ‹è¯•
    result = benchmark(array.shuffle, medium_array.copy(), iterations=100)
    print_benchmark_result("shuffle (1000 items)", result)
    
    # diff å‡½æ•°æµ‹è¯•
    array1 = list(range(500))
    array2 = list(range(250, 750))
    result = benchmark(array.diff, array1, array2, iterations=1000)
    print_benchmark_result("diff (500 vs 500 items)", result)


def benchmark_string_functions():
    """æµ‹è¯•å­—ç¬¦ä¸²å‡½æ•°æ€§èƒ½"""
    print("ğŸ“ å­—ç¬¦ä¸²å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®
    short_text = "hello_world_example"
    medium_text = "this_is_a_much_longer_string_with_many_words_and_underscores"
    long_text = "_".join([f"word{i}" for i in range(50)])
    
    # camel_case æµ‹è¯•
    result = benchmark(string.camel_case, medium_text, iterations=10000)
    print_benchmark_result("camel_case (medium text)", result)
    
    # snake_case æµ‹è¯•
    camel_text = "thisIsAVeryLongCamelCaseStringWithManyWords"
    result = benchmark(string.snake_case, camel_text, iterations=10000)
    print_benchmark_result("snake_case (camel text)", result)
    
    # slugify æµ‹è¯•
    complex_text = "Hello World! è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„å­—ç¬¦ä¸² 123 @#$%"
    result = benchmark(string.slugify, complex_text, iterations=5000)
    print_benchmark_result("slugify (complex text)", result)
    
    # fuzzy_match æµ‹è¯•
    result = benchmark(string.fuzzy_match, "hello world", "helo wrld", iterations=5000)
    print_benchmark_result("fuzzy_match (short strings)", result)
    
    # generate_uuid æµ‹è¯•
    result = benchmark(string.generate_uuid, iterations=10000)
    print_benchmark_result("generate_uuid", result)


def benchmark_math_functions():
    """æµ‹è¯•æ•°å­¦å‡½æ•°æ€§èƒ½"""
    print("ğŸ§® æ•°å­¦å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # clamp æµ‹è¯•
    result = benchmark(math.clamp, 150, 0, 100, iterations=100000)
    print_benchmark_result("clamp", result)
    
    # lerp æµ‹è¯•
    result = benchmark(math.lerp, 0, 100, 0.5, iterations=100000)
    print_benchmark_result("lerp", result)
    
    # normalize æµ‹è¯•
    result = benchmark(math.normalize, 75, 0, 100, iterations=100000)
    print_benchmark_result("normalize", result)
    
    # fibonacci æµ‹è¯•
    result = benchmark(math.fibonacci, 20, iterations=1000)
    print_benchmark_result("fibonacci(20)", result)
    
    # is_prime æµ‹è¯•
    result = benchmark(math.is_prime, 97, iterations=10000)
    print_benchmark_result("is_prime(97)", result)
    
    # gcd æµ‹è¯•
    result = benchmark(math.gcd, 48, 18, iterations=50000)
    print_benchmark_result("gcd(48, 18)", result)


def benchmark_object_functions():
    """æµ‹è¯•å¯¹è±¡å‡½æ•°æ€§èƒ½"""
    print("ğŸ—ï¸ å¯¹è±¡å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ•°æ®
    small_obj = {f"key{i}": f"value{i}" for i in range(10)}
    medium_obj = {f"key{i}": f"value{i}" for i in range(100)}
    large_obj = {f"key{i}": f"value{i}" for i in range(1000)}
    
    # pick æµ‹è¯•
    keys_to_pick = [f"key{i}" for i in range(0, 50, 5)]
    result = benchmark(object.pick, medium_obj, keys_to_pick, iterations=10000)
    print_benchmark_result("pick (100 items, pick 10)", result)
    
    # omit æµ‹è¯•
    keys_to_omit = [f"key{i}" for i in range(0, 20, 2)]
    result = benchmark(object.omit, medium_obj, keys_to_omit, iterations=10000)
    print_benchmark_result("omit (100 items, omit 10)", result)
    
    # merge æµ‹è¯•
    obj1 = {f"a{i}": i for i in range(50)}
    obj2 = {f"b{i}": i for i in range(50)}
    result = benchmark(object.merge, obj1, obj2, iterations=5000)
    print_benchmark_result("merge (50 + 50 items)", result)
    
    # deep_copy æµ‹è¯•
    nested_obj = {
        "level1": {
            "level2": {
                "level3": {f"key{i}": f"value{i}" for i in range(20)}
            }
        }
    }
    result = benchmark(object.deep_copy, nested_obj, iterations=5000)
    print_benchmark_result("deep_copy (nested object)", result)


def benchmark_function_utilities():
    """æµ‹è¯•å‡½æ•°å·¥å…·æ€§èƒ½"""
    print("âš™ï¸ å‡½æ•°å·¥å…·æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # memoize æµ‹è¯•
    @function.memoize
    def fibonacci_memo(n):
        if n <= 1:
            return n
        return fibonacci_memo(n-1) + fibonacci_memo(n-2)
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰
    start_time = time.perf_counter()
    result1 = fibonacci_memo(30)
    first_call_time = time.perf_counter() - start_time
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰
    start_time = time.perf_counter()
    result2 = fibonacci_memo(30)
    cached_call_time = time.perf_counter() - start_time
    
    print(f"  memoize fibonacci(30):")
    print(f"    é¦–æ¬¡è°ƒç”¨: {format_time(first_call_time)}")
    print(f"    ç¼“å­˜è°ƒç”¨: {format_time(cached_call_time)}")
    print(f"    åŠ é€Ÿæ¯”: {first_call_time / cached_call_time:.0f}x")
    print(f"    ç»“æœ: {result1}")
    print()
    
    # once è£…é¥°å™¨æµ‹è¯•
    call_count = 0
    
    @function.once
    def expensive_init():
        nonlocal call_count
        call_count += 1
        time.sleep(0.001)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        return "initialized"
    
    # å¤šæ¬¡è°ƒç”¨æµ‹è¯•
    start_time = time.perf_counter()
    for _ in range(1000):
        expensive_init()
    total_time = time.perf_counter() - start_time
    
    print(f"  once decorator (1000 calls):")
    print(f"    æ€»æ—¶é—´: {format_time(total_time)}")
    print(f"    å®é™…æ‰§è¡Œæ¬¡æ•°: {call_count}")
    print(f"    å¹³å‡æ¯æ¬¡è°ƒç”¨: {format_time(total_time / 1000)}")
    print()


async def benchmark_async_functions():
    """æµ‹è¯•å¼‚æ­¥å‡½æ•°æ€§èƒ½"""
    print("âš¡ å¼‚æ­¥å‡½æ•°æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # sleep_async æµ‹è¯•
    result = await async_benchmark(async_utils.sleep_async, 0.001, iterations=100)
    print_benchmark_result("sleep_async(0.001)", result)
    
    # delay æµ‹è¯•
    result = await async_benchmark(async_utils.delay, "test", 0.001, iterations=100)
    print_benchmark_result("delay('test', 0.001)", result)
    
    # map_async æµ‹è¯•
    async def simple_async_func(x):
        await asyncio.sleep(0.001)
        return x * 2
    
    items = list(range(10))
    start_time = time.perf_counter()
    result = await async_utils.map_async(simple_async_func, items, concurrency=5)
    total_time = time.perf_counter() - start_time
    
    print(f"  map_async (10 items, concurrency=5):")
    print(f"    æ€»æ—¶é—´: {format_time(total_time)}")
    print(f"    ç»“æœ: {result}")
    print(f"    å¹³å‡æ¯é¡¹: {format_time(total_time / len(items))}")
    print()
    
    # race æµ‹è¯•
    async def fast_task():
        await asyncio.sleep(0.001)
        return "fast"
    
    async def slow_task():
        await asyncio.sleep(0.01)
        return "slow"
    
    start_time = time.perf_counter()
    winner = await async_utils.race(fast_task(), slow_task())
    race_time = time.perf_counter() - start_time
    
    print(f"  race (fast vs slow):")
    print(f"    æ€»æ—¶é—´: {format_time(race_time)}")
    print(f"    è·èƒœè€…: {winner}")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ pyutils æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print()
    
    try:
        # åŒæ­¥å‡½æ•°æµ‹è¯•
        benchmark_array_functions()
        benchmark_string_functions()
        benchmark_math_functions()
        benchmark_object_functions()
        benchmark_function_utilities()
        
        # å¼‚æ­¥å‡½æ•°æµ‹è¯•
        print("å¼€å§‹å¼‚æ­¥å‡½æ•°æ€§èƒ½æµ‹è¯•...")
        asyncio.run(benchmark_async_functions())
        
        print("âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print("\nğŸ“Š æµ‹è¯•æ€»ç»“:")
        print("- æ‰€æœ‰å‡½æ•°éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ç‰¹å¾")
        print("- memoize è£…é¥°å™¨æ˜¾è‘—æå‡äº†é‡å¤è®¡ç®—çš„æ€§èƒ½")
        print("- å¼‚æ­¥å‡½æ•°åœ¨å¹¶å‘åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚")
        print("- å­—ç¬¦ä¸²å’Œæ•°ç»„æ“ä½œé’ˆå¯¹ä¸åŒè§„æ¨¡çš„æ•°æ®éƒ½æœ‰åˆç†çš„æ€§èƒ½")
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()